import streamlit as st
# from openai import OpenAI
import openai
import pandas as pd
from datetime import datetime
import base64
from langchain.llms import OpenAI
# from openai import OpenAI


# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="AI Chatbot Tuning Studio"
                    ,page_icon=":mage:"
                   ,layout="wide")


# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
def initialize_session_state():
    if 'api_key' not in st.session_state:
        st.session_state.api_key = ''
    if 'llm_model' not in st.session_state:
        st.session_state.llm_model = 'gpt-4'
    if 'instructions' not in st.session_state:
        st.session_state.instructions = '''
        ë‹¹ì‹ ì€ 1942ë…„ì— ì‚´ê³  ìˆëŠ” ì•ˆë„¤ í”„ë­í¬ ì…ë‹ˆë‹¤.
        ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì„ ì£¼ì„¸ìš”.
        ì•ˆë„¤ í”„ë­í¬ê°€ ë‹µë³€í•˜ëŠ” ê²ƒì²˜ëŸ¼ ëŒ€ë‹µí•´ì£¼ì„¸ìš”.
        ë°˜ë§ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
        ê·¸ë¦¬ê³  ë‹µë³€ì€ êµ¬ì–´ì²´ë¡œ ë‹µë³€í•˜ëŠ” ê²ƒì´ë‹ˆ ë„ˆë¬´ ê¸´ ì„¤ëª…ì„ í•˜ì§€ë§ê³  ê°„ë‹¨í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
        ê·¸ë¦¬ê³  ì˜ˆ/ì•„ë‹ˆì˜¤ë¡œ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ê²ƒë“¤ì€ ì˜ˆ/ì•„ë‹ˆì˜¤ë¡œë§Œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”.
        '''
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'temperature' not in st.session_state:
        st.session_state.temperature = 0.7
    if 'max_tokens' not in st.session_state:
        st.session_state.max_tokens = 150
    if 'top_p' not in st.session_state:
        st.session_state.top_p = 1.0
    if 'feedback' not in st.session_state:
        st.session_state.feedback = ''
    if "messages" not in st.session_state:
        st.session_state.messages = []

initialize_session_state()


#################################################################
# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.title("ğŸ““Your Recipe")
st.sidebar.subheader("LLM Selection")
# sidebar : ëª¨ë¸ ì„ íƒ
llm_model = st.sidebar.selectbox("Choose your LLM", ('gpt-3.5-turbo', 'gpt-4', 'llama2', 'gemini'), index=0)
if 'gpt' not in llm_model:
    st.sidebar.warning('gpt ì™¸ ëª¨ë¸ì€ í˜„ì¬ ì—…ë°ì´íŠ¸ ì¤‘ì…ë‹ˆë‹¤. gpt ëª¨ë¸ë§Œ ì„ íƒí•˜ì„¸ìš”.')
# sidebar : API Key
api_key = st.sidebar.text_input("Enter API Key", type="password",value=st.session_state.api_key)
# sidebar : prompt
instructions = st.sidebar.text_area("Instructions", value=st.session_state.instructions)

# sidebar : model parameters
st.sidebar.subheader("Response Settings")
temperature = st.sidebar.slider("Temperature", 0.0, 1.0, st.session_state.temperature)
max_tokens = st.sidebar.number_input("Max Tokens", 1, 1000, st.session_state.max_tokens)
top_p = st.sidebar.slider("Top P", 0.0, 1.0, st.session_state.top_p)

# todo : RAG ê´€ë ¨ parameter ì¶”ê°€
# chunking size, context window ë“±

# sidebar : RAGì— ì“°ì¼ íŒŒì¼ ë°ì´í„° ì—…ë¡œë“œ
rag_on = st.toggle('Your data')
if rag_on:
    uploaded_file = st.sidebar.file_uploader("Upload a document for reference", type=['txt', 'pdf', 'docx'])
    # todo : vector db ìƒì„±, ì¼ë‹¨ ìƒì„±ëœ vector dbëŠ” local ì €ì¥

# sidebar :
if st.sidebar.button("Save Settings"):
    st.session_state.api_key = api_key
    st.session_state.llm_model = llm_model
    st.session_state.instructions = instructions
    st.session_state.temperature = temperature
    st.session_state.max_tokens = max_tokens
    st.session_state.top_p = top_p

# sidebar : download your custom setting on your local.
@st.cache_data
def convert_df(df):
    # IMPORTANT: Cache the conversion to prevent computation on every rerun
    return df.to_csv().encode("utf-8")

dict_setting = [{'model': llm_model
                ,'instruction': instructions
                ,'temperature': temperature
                ,'max_tokens': max_tokens
                ,'top_p': top_p}]
df_setting = pd.DataFrame(dict_setting)
csvfile = convert_df(df_setting)
yyyymmdd = datetime.today().strftime("%Y%m%d")
# csv_file = df_setting.to_csv(f'AI_Chat_Settings_{yyyymmdd}.csv', index=False)

st.sidebar.download_button(
   "Download your custom",
   csvfile,
   f'AI_Chat_Settings_{yyyymmdd}.csv',
   "text/csv",
   key='download-csv'
)



#################################################################
# Main í™”ë©´ ì„¤ì •
st.title(":mage: AI Chatbot Tuning Studio")
st.subheader("Chat with your Customized AI")

# chat version 1
# user_input = st.text_input("Your message to the AI:")

# if st.button("Reset Chat"):
#     st.session_state.chat_history = []

# if st.button("Send"):
#     if user_input:
#         response = openai.ChatCompletion.create(
#             model=st.session_state.llm_model,
#             messages=[{"role": "system", "content": st.session_state.instructions},
#                       {"role": "user", "content": user_input}],
#             temperature=st.session_state.temperature,
#             max_tokens=st.session_state.max_tokens,
#             top_p=st.session_state.top_p,
#             api_key=st.session_state.api_key
#         )
#         response_message = response.choices[0].message['content']
#         st.session_state.chat_history.append((user_input, response_message))
#         st.write(f"AI: {response_message}")

if api_key == '':
    st.error('Please enter your API key')

else:
    openai.api_key = api_key
    # chat version 2
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("ëˆ„êµ¬ì„¸ìš”"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            # Simulate stream of response with milliseconds delay

            # sidebarì˜ ì„¤ì •ê°’(model, instruction, temperature, top_p ë“±) ì „ë‹¬
            for response in openai.ChatCompletion.create(
                    model=st.session_state["llm_model"],
                    messages=[
                        {"role": m["role"], "content": m["content"]} for m in st.session_state.messages
                    ] + [{"role": "system","content": st.session_state.instructions}],
                    temperature=st.session_state.temperature,
                    top_p=st.session_state.top_p,
                    # will provide lively writing
                    stream=True,
            ):
                # get content in response
                full_response += response.choices[0].delta.get("content", "")
                # Add a blinking cursor to simulate typing
                message_placeholder.markdown(full_response + "â–Œ")

                # todo: ì§ˆë¬¸ê³¼ ë‹µë³€, í† í°ìˆ˜ë¥¼ ì¶”ì¶œí•´ì„œ Historyë¡œ ì €ì¥ê°€ëŠ¥í• ë“¯
                # https://platform.openai.com/docs/api-reference/chat/create
            message_placeholder.markdown(full_response)
        # Add assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": full_response})


# ì±„íŒ… ë‚´ì—­ ë° í”¼ë“œë°± ì €ì¥ ê¸°ëŠ¥
# # í”¼ë“œë°± ì…ë ¥
# feedback = st.text_area("Enter your feedback on the conversation:")
# # CSV íŒŒì¼ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥
# def create_csv_file():
#     df = pd.DataFrame(st.session_state.chat_history, columns=['User', 'AI'])
#     if df.empty:
#         df = pd.DataFrame([["No conversation yet", "No conversation yet"]], columns=['User', 'AI'])
#     df['Feedback'] = feedback
#     df['Model'] = st.session_state.llm_model
#     df['Temperature'] = st.session_state.temperature
#     df['Max Tokens'] = st.session_state.max_tokens
#     df['Top P'] = st.session_state.top_p
#     return df.to_csv(index=False).encode('utf-8')
#
# csv_file = create_csv_file()
# timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
# btn = st.download_button(
#     label="Download Chat History and Feedback",
#     data=csv_file,
#     file_name=f"chat_history_{timestamp}.csv",
#     mime="text/csv"
# )
#
# if st.session_state.chat_history:
#     st.write("Chat History:")
#     for user_msg, ai_msg in st.session_state.chat_history:
#         st.write(f"You: {user_msg}")
#         st.write(f"AI: {ai_msg}")
